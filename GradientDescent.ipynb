{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Se importa numpy para facilitar las operaciones con matrices\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "#Descenso estocástico de gradiente para regresión lineal.\n",
    "#training_data_X es una matriz que representa el conjunto de x_i's\n",
    "#training_data_y es una matriz que representa el conjunto de y_i's. Esta matriz tiene únicamente una \n",
    "#Pre: training_data_X  y training_data_y deben tener el mismo orden, es decir, para todo elemento X_i \n",
    "# de la matriz training_data_X, y_i en la matriz training_data_y debe ser su respectiva pareja.\n",
    "# ejemplo, la fila 0 de training_data_X tiene como respectivo valor y el que está dado por \n",
    "# la fila 0 de training_data_y\n",
    "def sgd(training_data_X, training_data_y,learning_rate= 0.001, max_iterations = 10000):\n",
    "    \n",
    "    #Para evitar que la embarre y ponga un learning rate demasiado alto que lleve a que el método no converja \n",
    "    if(learning_rate > 0.0001):\n",
    "        learning_rate = 0.0001\n",
    "    \n",
    "    #Se inicializa W en ceros, de dimensión (columnas X, 1).\n",
    "    #Se hace así para que no se tenga que transponer W.\n",
    "    W = np.zeros(shape = (1, len(training_data_X[0])))\n",
    "    \n",
    "    iterations = 0\n",
    "    no_improv_it = 0\n",
    "    last_error = 0\n",
    "    new_error = 0\n",
    "    \n",
    "    #Se estimará el gradiente a partir, únicamente, de uno de los datos\n",
    "    while iterations < max_iterations and no_improv_it < 100 :\n",
    "        \n",
    "        #Seleccionar un dato al azar.\n",
    "        i = math.floor(random.random()*len(training_data_X))\n",
    "    \n",
    "        #Calcular error del dato seleccionado al azar.\n",
    "        e = np.dot(training_data_X[i], W.transpose())[0] - training_data_y[i]\n",
    "        #Estimar gradiente con el error anterior\n",
    "        estimated_grad = np.multiply(e, training_data_X[i])\n",
    "        \n",
    "        W = W - np.multiply(learning_rate, estimated_grad)\n",
    "        \n",
    "        new_error = residual_sum_of_squares(W,training_data_X, training_data_y)\n",
    "    \n",
    "        #Si no hubo una mejora en el error cuadrático, reducir tasa de aprendizaje (puede que se esté \"saltando\" el punto óptimo)\n",
    "        if last_error < new_error:\n",
    "            learning_rate*=0.9\n",
    "            no_improv_it = 0\n",
    "        elif last_error == new_error:\n",
    "            no_improv_it += 1\n",
    "        else:\n",
    "            no_improv_it = 0\n",
    "        \n",
    "        last_error = new_error\n",
    "        iterations += 1        \n",
    "    \n",
    "    print(\"----------------------------------------------\")\n",
    "    print(\"SDG finished, best residual_sum_of_squares %s\" % last_error)\n",
    "    print(\"W found: %s\" % W)\n",
    "    print(\" R^2 found for the model: %s\" % rcuadrado(W, training_data_X, training_data_y))\n",
    "\n",
    "def sgd_large_dataset(training_data_X, training_data_y,learning_rate= 0.001, max_iterations = 10000):\n",
    "    \n",
    "    #Para evitar que la embarre y ponga un learning rate demasiado alto que lleve a que el método no converja \n",
    "    if(learning_rate > 0.0001):\n",
    "        learning_rate = 0.0001\n",
    "    \n",
    "    #Se inicializa W en ceros, de dimensión (columnas X, 1).\n",
    "    #Se hace así para que no se tenga que transponer W.\n",
    "    W = np.zeros(shape = (1, len(training_data_X[0])))\n",
    "    \n",
    "    iterations = 0\n",
    "    no_improv_it = 0\n",
    "    last_error = 0\n",
    "    new_error = 0\n",
    "    \n",
    "    #Se estimará el gradiente a partir, únicamente, de uno de los datos\n",
    "    while iterations < max_iterations and no_improv_it < 100 :\n",
    "        \n",
    "        #Seleccionar un dato al azar.\n",
    "        i = math.floor(random.random()*len(training_data_X))\n",
    "    \n",
    "        #Calcular error del dato seleccionado al azar.\n",
    "        e = np.dot(training_data_X[i], W.transpose())[0] - training_data_y[i]\n",
    "        #Estimar gradiente con el error anterior\n",
    "        estimated_grad = np.multiply(e, training_data_X[i])\n",
    "        \n",
    "        W = W - np.multiply(learning_rate, estimated_grad)\n",
    "            \n",
    "        #Si no hubo una mejora en el error cuadrático, reducir tasa de aprendizaje (puede que se esté \"saltando\" el punto óptimo)\n",
    "        if last_error < e:\n",
    "            learning_rate*=0.9\n",
    "            no_improv_it = 0\n",
    "        elif last_error == e:\n",
    "            no_improv_it += 1\n",
    "        else:\n",
    "            no_improv_it = 0\n",
    "        \n",
    "        last_error = e\n",
    "        iterations += 1        \n",
    "    \n",
    "    print(\"----------------------------------------------\")\n",
    "    print(\"SDG finished, best residual_sum_of_squares %s\" % last_error)\n",
    "    print(\"W found: %s\" % W)\n",
    "    print(\" R^2 found for the model: %s\" % rcuadrado(W, training_data_X, training_data_y))\n",
    "           \n",
    "    \n",
    "def rcuadrado(W, test_data_X, test_data_y):\n",
    "    \n",
    "    residual_sum = residual_sum_of_squares(W, test_data_X, test_data_y)\n",
    "    \n",
    "    mean = np.mean(test_data_y)\n",
    "        \n",
    "    total_sum_squares = 0\n",
    "    for i in range(0, len(test_data_y)):\n",
    "        total_sum_squares +=  math.pow(test_data_y[i] - mean,2)\n",
    "    \n",
    "    print(residual_sum)\n",
    "    print(total_sum_squares)\n",
    "\n",
    "    \n",
    "    return 1 - residual_sum/total_sum_squares\n",
    "\n",
    "    \n",
    "        \n",
    "#Calcula el error de la función, se usa sólo una vez por iteración.\n",
    "def residual_sum_of_squares(W, training_data_X, training_data_y):\n",
    "    \n",
    "    sum = 0\n",
    "    for i in range(0, len(training_data_X)):\n",
    "        sum+= math.pow(training_data_y[i] - np.dot(training_data_X[i] , W.transpose())[0],2)\n",
    "    \n",
    "    return 0.5 * sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pruebas rápidas del método:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas de la matriz: 4898\n",
      "Columnas de la matriz: 12\n",
      "[[ 7.       0.27     0.36    ...  3.       0.45     8.8    ]\n",
      " [ 6.       6.3      0.3     ...  0.994    3.3      0.49   ]\n",
      " [ 9.5      6.       8.1     ... 97.       0.9951   3.26   ]\n",
      " ...\n",
      " [ 0.13     0.28     0.9     ...  0.52    11.2      6.     ]\n",
      " [ 6.7      0.48     0.49    ...  3.13     0.4     13.     ]\n",
      " [ 6.       6.7      0.48    ...  0.98926  3.13     0.4    ]]\n",
      "[6. 6. 6. ... 6. 7. 6.]\n"
     ]
    }
   ],
   "source": [
    "data_matrix = np.loadtxt(open(\"./WineQuality/winequality-white.csv\", \"rb\"), delimiter=\";\", skiprows=1)\n",
    "print(\"Filas de la matriz: \" + str(len(data_matrix)))\n",
    "print(\"Columnas de la matriz: \" + str(len(data_matrix[0])))\n",
    "\n",
    "X = np.resize(data_matrix, (len(data_matrix), len(data_matrix[0])-1))\n",
    "y = data_matrix[:,11]\n",
    "\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "SDG finished, best residual_sum_of_squares 12385.973466999734\n",
      "W found: [[0.04022056 0.01843098 0.02762465 0.04211826 0.01571573 0.02872201\n",
      "  0.02644516 0.02270781 0.03372551 0.02253315 0.02959164]]\n",
      "12385.973466999734\n",
      "3840.989791751859\n",
      " R^2 found for the model: -2.2246827350589093\n"
     ]
    }
   ],
   "source": [
    "sgd(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pruebas con un set de datos que sé que sí sirve para regresión lineal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "SDG finished, best residual_sum_of_squares 2473.2461019723005\n",
      "W found: [[0.00847107]]\n",
      "2473.2461019723005\n",
      "5060.6524237669555\n",
      " R^2 found for the model: 0.5112791998207791\n"
     ]
    }
   ],
   "source": [
    "import mglearn\n",
    "X, y = mglearn.datasets.make_wave(n_samples=5000)\n",
    "sgd(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas de la matriz: 96453\n",
      "Columnas de la matriz: 8\n",
      "[[9.05579391e-03 1.34989541e-02 2.39965260e-01 ... 0.00000000e+00\n",
      "  9.70501732e-01 7.06405038e-03]\n",
      " [8.48918419e-04 8.92371172e-03 1.36061592e-02 ... 1.50957726e-02\n",
      "  0.00000000e+00 9.68749454e-01]\n",
      " [3.52695969e-02 4.19656694e-03 4.57610142e-02 ... 9.95464717e-01\n",
      "  7.29856187e-02 0.00000000e+00]\n",
      " ...\n",
      " [9.00047384e-02 3.31159719e-03 9.73277692e-02 ... 9.73999173e-01\n",
      "  1.00779045e-01 0.00000000e+00]\n",
      " [9.87377024e-01 1.45763744e-02 4.47340179e-04 ... 2.24520036e-02\n",
      "  1.54624105e-01 1.47174919e-02]\n",
      " [0.00000000e+00 9.90067869e-01 1.56932152e-02 ... 1.67556919e-02\n",
      "  2.40355007e-02 1.36604153e-01]]\n",
      "[0.89 0.86 0.89 ... 0.56 0.6  0.61]\n"
     ]
    }
   ],
   "source": [
    "data_matrix = np.loadtxt(open(\"./szeged-weather/weatherHistory.csv\", \"rb\"), delimiter=\",\", skiprows=1)\n",
    "print(\"Filas de la matriz: \" + str(len(data_matrix)))\n",
    "print(\"Columnas de la matriz: \" + str(len(data_matrix[0])))\n",
    "\n",
    "X = np.resize(data_matrix, (len(data_matrix), len(data_matrix[0])-1))\n",
    "y = data_matrix[:,len(data_matrix[0])-1]\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "X = normalize(X)\n",
    "\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "SDG finished, best residual_sum_of_squares -0.2996184730085636\n",
      "W found: [[0.0003061  0.00015195 0.00029239 0.00027839 0.00043315 0.00023312\n",
      "  0.00018452]]\n",
      "27865.096466011073\n",
      "3685.391540427899\n",
      " R^2 found for the model: -6.560959577927436\n",
      "0.0003938931136273638\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "sgd_large_dataset(X, y)\n",
    "print(LinearRegression().fit(X, y).score(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas de la matriz: 489\n",
      "Columnas de la matriz: 4\n",
      "[[3.78272947e-01 2.86509395e-01 8.80239708e-01]\n",
      " [1.00000000e+00 1.27400794e-05 1.81349206e-05]\n",
      " [3.92416225e-05 9.99999999e-01 1.58399471e-05]\n",
      " ...\n",
      " [8.82481429e-05 9.99999996e-01 2.41852336e-05]\n",
      " [9.94521694e-05 8.51243145e-05 9.99999991e-01]\n",
      " [1.71027880e-01 7.60950132e-01 6.25862893e-01]]\n",
      "[ 504000.  453600.  728700.  701400.  760200.  602700.  480900.  569100.\n",
      "  346500.  396900.  315000.  396900.  455700.  428400.  382200.  417900.\n",
      "  485100.  367500.  424200.  382200.  285600.  411600.  319200.  304500.\n",
      "  327600.  291900.  348600.  310800.  386400.  441000.  266700.  304500.\n",
      "  277200.  275100.  283500.  396900.  420000.  441000.  518700.  646800.\n",
      "  732900.  558600.  531300.  518700.  445200.  405300.  420000.  348600.\n",
      "  302400.  407400.  413700.  430500.  525000.  491400.  396900.  743400.\n",
      "  518700.  663600.  489300.  411600.  392700.  336000.  466200.  525000.\n",
      "  693000.  493500.  407400.  462000.  365400.  438900.  508200.  455700.\n",
      "  478800.  491400.  506100.  449400.  420000.  436800.  445200.  426300.\n",
      "  588000.  501900.  520800.  480900.  501900.  558600.  472500.  466200.\n",
      "  495600.  602700.  474600.  462000.  480900.  525000.  432600.  596400.\n",
      "  449400.  812700.  919800.  697200.  577500.  556500.  390600.  405300.\n",
      "  422100.  409500.  409500.  428400.  415800.  407400.  455700.  478800.\n",
      "  394800.  392700.  388500.  384300.  445200.  403200.  428400.  405300.\n",
      "  462000.  426300.  430500.  363300.  394800.  449400.  329700.  340200.\n",
      "  378000.  300300.  403200.  411600.  483000.  386400.  327600.  380100.\n",
      "  365400.  359100.  279300.  373800.  294000.  302400.  281400.  327600.\n",
      "  247800.  289800.  327600.  306600.  373800.  323400.  451500.  411600.\n",
      "  321300.  407400.  357000.  327600.  275100.  867300.  510300.  489300.\n",
      "  567000.  476700.  525000.  499800.  499800.  468300.  365400.  401100.\n",
      "  485100.  495600.  474600.  617400.  487200.  516600.  627900.  781200.\n",
      "  835800.  760200.  795900.  682500.  554400.  621600.  672000.  625800.\n",
      "  732900.  777000.  640500.  764400.  653100.  611100.  699300.  636300.\n",
      "  726600.  732900.  690900.  506100.  888300. 1018500.  474600.  512400.\n",
      "  472500.  512400.  420000.  455700.  405300.  470400.  590100.  497700.\n",
      "  525000.  489300.  602700.  451500.  483000.  560700.  455700.  577500.\n",
      "  632100.  940800.  789600.  663600.  980700.  661500.  510300.  665700.\n",
      "  875700. 1014300.  609000.  504000.  527100.  661500.  497700.  489300.\n",
      "  462000.  422100.  466200.  497700.  369600.  388500.  510300.  430500.\n",
      "  514500.  550200.  512400.  520800.  621600.  898800.  459900.  438900.\n",
      "  924000.  756000.  632100.  709800.  905100. 1024800.  651000.  766500.\n",
      "  478800.  644700.  913500.  434700.  443100.  529200.  512400.  739200.\n",
      "  680400.  672000.  697200.  695100.  611100.  737100.  953400.  743400.\n",
      "  966000.  676200.  462000.  422100.  487200.  468300.  520800.  598500.\n",
      "  783300.  585900.  501900.  455700.  600600.  569100.  426300.  472500.\n",
      "  609000.  520800.  462000.  554400.  695100.  758100.  596400.  701400.\n",
      "  592200.  478800.  426300.  338100.  464100.  407400.  453600.  499800.\n",
      "  340200.  373800.  415800.  485100.  441000.  499800.  485100.  428400.\n",
      "  388500.  525000.  516600.  483000.  466200.  405300.  474600.  415800.\n",
      "  359100.  407400.  466200.  434700.  443100.  409500.  388500.  432600.\n",
      "  399000.  392700.  686700.  346500.  501900.  655200.  367500.  361200.\n",
      "  485100.  514500.  558600.  480900.  506100.  390600.  632100.  382200.\n",
      "  432600.  373800.  455700.  476700.  474600.  525000.  417900.  436800.\n",
      "  352800.  577500.  459900.  485100.  289800.  289800.  315000.  291900.\n",
      "  279300.  275100.  214200.  218400.  228900.  237300.  258300.  184800.\n",
      "  151200.  220500.  155400.  214200.  241500.  317100.  487200.  203700.\n",
      "  289800.  266700.  275100.  262500.  178500.  105000.  132300.  117600.\n",
      "  151200.  254100.  174300.  178500.  105000.  249900.  585900.  361200.\n",
      "  577500.  315000.  361200.  375900.  342300.  147000.  151200.  157500.\n",
      "  218400.  184800.  176400.  350700.  298200.  436800.  281400.  245700.\n",
      "  174300.  214200.  228900.  231000.  199500.  304500.  296100.  338100.\n",
      "  300300.  245700.  281400.  201600.  182700.  176400.  268800.  220500.\n",
      "  359100.  386400.  323400.  226800.  247800.  312900.  264600.  296100.\n",
      "  273000.  281400.  319200.  338100.  373800.  312900.  296100.  266700.\n",
      "  283500.  312900.  420000.  344400.  371700.  409500.  424200.  449400.\n",
      "  417900.  399000.  401100.  401100.  422100.  417900.  411600.  487200.\n",
      "  625800.  289800.  279300.  350700.  252000.  306600.  449400.  483000.\n",
      "  497700.  525000.  457800.  432600.  445200.  401100.  432600.  319200.\n",
      "  147000.  170100.  285600.  422100.  457800.  514500.  485100.  413700.\n",
      "  384300.  445200.  367500.  352800.  470400.  432600.  501900.  462000.\n",
      "  249900.]\n"
     ]
    }
   ],
   "source": [
    "data_matrix = np.loadtxt(open(\"./bostonhoustingmlnd/housing.csv\", \"rb\"), delimiter=\",\", skiprows=1)\n",
    "print(\"Filas de la matriz: \" + str(len(data_matrix)))\n",
    "print(\"Columnas de la matriz: \" + str(len(data_matrix[0])))\n",
    "\n",
    "X = np.resize(data_matrix, (len(data_matrix), len(data_matrix[0])-1))\n",
    "y = data_matrix[:,len(data_matrix[0])-1]\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "X = normalize(X)\n",
    "\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "SDG finished, best residual_sum_of_squares 35410831518667.43\n",
      "W found: [[ 85860.20149609  95549.2304364  110783.67523526]]\n",
      "35410831518667.43\n",
      "13340654818159.514\n",
      " R^2 found for the model: -1.6543548275055913\n",
      "0.004591194163290235\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sgd(X, y)\n",
    "print(LinearRegression().fit(X, y).score(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas de la matriz: 505\n",
      "Columnas de la matriz: 14\n",
      "[[2.73100e-02 0.00000e+00 7.07000e+00 ... 1.78000e+01 3.96900e+02\n",
      "  9.14000e+00]\n",
      " [2.16000e+01 2.72900e-02 0.00000e+00 ... 2.42000e+02 1.78000e+01\n",
      "  3.92830e+02]\n",
      " [4.03000e+00 3.47000e+01 3.23700e-02 ... 3.00000e+00 2.22000e+02\n",
      "  1.87000e+01]\n",
      " ...\n",
      " [1.81000e+01 0.00000e+00 5.84000e+02 ... 2.13200e+01 1.91000e+01\n",
      "  1.55757e+01]\n",
      " [0.00000e+00 1.81000e+01 0.00000e+00 ... 3.68740e+02 1.81300e+01\n",
      "  1.91000e+01]\n",
      " [1.30751e+01 0.00000e+00 1.81000e+01 ... 2.02000e+01 3.96900e+02\n",
      "  1.47600e+01]]\n",
      "[21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 15.  18.9 21.7 20.4 18.2\n",
      " 19.9 23.1 17.5 20.2 18.2 13.6 19.6 15.2 14.5 15.6 13.9 16.6 14.8 18.4\n",
      " 21.  12.7 14.5 13.2 13.1 13.5 18.9 20.  21.  24.7 30.8 34.9 26.6 25.3\n",
      " 24.7 21.2 19.3 20.  16.6 14.4 19.4 19.7 20.5 25.  23.4 18.9 35.4 24.7\n",
      " 31.6 23.3 19.6 18.7 16.  22.2 25.  33.  23.5 19.4 22.  17.4 20.9 24.2\n",
      " 21.7 22.8 23.4 24.1 21.4 20.  20.8 21.2 20.3 28.  23.9 24.8 22.9 23.9\n",
      " 26.6 22.5 22.2 23.6 28.7 22.6 22.  22.9 25.  20.6 28.4 21.4 38.7 43.8\n",
      " 33.2 27.5 26.5 18.6 19.3 20.1 19.5 19.5 20.4 19.8 19.4 21.7 22.8 18.8\n",
      " 18.7 18.5 18.3 21.2 19.2 20.4 19.3 22.  20.3 20.5 17.3 18.8 21.4 15.7\n",
      " 16.2 18.  14.3 19.2 19.6 23.  18.4 15.6 18.1 17.4 17.1 13.3 17.8 14.\n",
      " 14.4 13.4 15.6 11.8 13.8 15.6 14.6 17.8 15.4 21.5 19.6 15.3 19.4 17.\n",
      " 15.6 13.1 41.3 24.3 23.3 27.  50.  50.  50.  22.7 25.  50.  23.8 23.8\n",
      " 22.3 17.4 19.1 23.1 23.6 22.6 29.4 23.2 24.6 29.9 37.2 39.8 36.2 37.9\n",
      " 32.5 26.4 29.6 50.  32.  29.8 34.9 37.  30.5 36.4 31.1 29.1 50.  33.3\n",
      " 30.3 34.6 34.9 32.9 24.1 42.3 48.5 50.  22.6 24.4 22.5 24.4 20.  21.7\n",
      " 19.3 22.4 28.1 23.7 25.  23.3 28.7 21.5 23.  26.7 21.7 27.5 30.1 44.8\n",
      " 50.  37.6 31.6 46.7 31.5 24.3 31.7 41.7 48.3 29.  24.  25.1 31.5 23.7\n",
      " 23.3 22.  20.1 22.2 23.7 17.6 18.5 24.3 20.5 24.5 26.2 24.4 24.8 29.6\n",
      " 42.8 21.9 20.9 44.  50.  36.  30.1 33.8 43.1 48.8 31.  36.5 22.8 30.7\n",
      " 50.  43.5 20.7 21.1 25.2 24.4 35.2 32.4 32.  33.2 33.1 29.1 35.1 45.4\n",
      " 35.4 46.  50.  32.2 22.  20.1 23.2 22.3 24.8 28.5 37.3 27.9 23.9 21.7\n",
      " 28.6 27.1 20.3 22.5 29.  24.8 22.  26.4 33.1 36.1 28.4 33.4 28.2 22.8\n",
      " 20.3 16.1 22.1 19.4 21.6 23.8 16.2 17.8 19.8 23.1 21.  23.8 23.1 20.4\n",
      " 18.5 25.  24.6 23.  22.2 19.3 22.6 19.8 17.1 19.4 22.2 20.7 21.1 19.5\n",
      " 18.5 20.6 19.  18.7 32.7 16.5 23.9 31.2 17.5 17.2 23.1 24.5 26.6 22.9\n",
      " 24.1 18.6 30.1 18.2 20.6 17.8 21.7 22.7 22.6 25.  19.9 20.8 16.8 21.9\n",
      " 27.5 21.9 23.1 50.  50.  50.  50.  50.  13.8 13.8 15.  13.9 13.3 13.1\n",
      " 10.2 10.4 10.9 11.3 12.3  8.8  7.2 10.5  7.4 10.2 11.5 15.1 23.2  9.7\n",
      " 13.8 12.7 13.1 12.5  8.5  5.   6.3  5.6  7.2 12.1  8.3  8.5  5.  11.9\n",
      " 27.9 17.2 27.5 15.  17.2 17.9 16.3  7.   7.2  7.5 10.4  8.8  8.4 16.7\n",
      " 14.2 20.8 13.4 11.7  8.3 10.2 10.9 11.   9.5 14.5 14.1 16.1 14.3 11.7\n",
      " 13.4  9.6  8.7  8.4 12.8 10.5 17.1 18.4 15.4 10.8 11.8 14.9 12.6 14.1\n",
      " 13.  13.4 15.2 16.1 17.8 14.9 14.1 12.7 13.5 14.9 20.  16.4 17.7 19.5\n",
      " 20.2 21.4 19.9 19.  19.1 19.1 20.1 19.9 19.6 23.2 29.8 13.8 13.3 16.7\n",
      " 12.  14.6 21.4 23.  23.7 25.  21.8 20.6 21.2 19.1 20.6 15.2  7.   8.1\n",
      " 13.6 20.1 21.8 24.5 23.1 19.7 18.3 21.2 17.5 16.8 22.4 20.6 23.9 22.\n",
      " 11.9]\n"
     ]
    }
   ],
   "source": [
    "data_matrix = np.loadtxt(open(\"./BostonHousing_preprocessed.csv\", \"r\", encoding=\"utf-8\"), delimiter=\",\", skiprows=1)\n",
    "print(\"Filas de la matriz: \" + str(len(data_matrix)))\n",
    "print(\"Columnas de la matriz: \" + str(len(data_matrix[0])))\n",
    "X = np.resize(data_matrix, (len(data_matrix), len(data_matrix[0])-1))\n",
    "y = data_matrix[:,len(data_matrix[0])-1]\n",
    "print(X)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "SDG finished, best residual_sum_of_squares 1.0979158658993548e+116\n",
      "W found: [[-6.44744552e+52  1.06029957e+53  1.73609848e+53 -3.56394706e+52\n",
      "   3.65127475e+52 -9.47775179e+52  2.53179660e+53 -5.70792112e+51\n",
      "  -8.37659693e+52  1.81109360e+52  1.69085492e+53 -9.62280881e+52\n",
      "   3.88578166e+51]]\n",
      "1.0979158658993548e+116\n",
      "42714.138495049454\n",
      " R^2 found for the model: -2.570380451490559e+111\n",
      "0.005382807398750189\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "sgd(X, y)\n",
    "print(LinearRegression().fit(X, y).score(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
