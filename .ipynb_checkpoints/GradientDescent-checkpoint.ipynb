{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Se importa numpy para facilitar las operaciones con matrices\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "#Descenso estocástico de gradiente para regresión lineal.\n",
    "#training_data_X es una matriz que representa el conjunto de x_i's\n",
    "#training_data_y es una matriz que representa el conjunto de y_i's. Esta matriz tiene únicamente una \n",
    "#Pre: training_data_X  y training_data_y deben tener el mismo orden, es decir, para todo elemento X_i \n",
    "# de la matriz training_data_X, y_i en la matriz training_data_y debe ser su respectiva pareja.\n",
    "# ejemplo, la fila 0 de training_data_X tiene como respectivo valor y el que está dado por \n",
    "# la fila 0 de training_data_y\n",
    "def sgd(training_data_X, training_data_y,learning_rate= 0.001, max_iterations = 10000):\n",
    "    \n",
    "    #Para evitar que la embarre y ponga un learning rate demasiado alto que lleve a que el método no converja \n",
    "    if(learning_rate > 0.0001):\n",
    "        learning_rate = 0.0001\n",
    "    \n",
    "    #Se inicializa W en ceros, de dimensión (columnas X, 1).\n",
    "    #Se hace así para que no se tenga que transponer W.\n",
    "    W = np.zeros(shape = (1, len(training_data_X[0])))\n",
    "    \n",
    "    iterations = 0\n",
    "    no_improv_it = 0\n",
    "    last_error = 0\n",
    "    new_error = 0\n",
    "    \n",
    "    #Se estimará el gradiente a partir, únicamente, de uno de los datos\n",
    "    while iterations < max_iterations and no_improv_it < 100 :\n",
    "        \n",
    "        #Seleccionar un dato al azar.\n",
    "        i = math.floor(random.random()*len(training_data_X))\n",
    "    \n",
    "        #Calcular error del dato seleccionado al azar.\n",
    "        e = np.dot(training_data_X[i], W.transpose())[0] - training_data_y[i]\n",
    "        #Estimar gradiente con el error anterior\n",
    "        estimated_grad = np.multiply(e, training_data_X[i])\n",
    "        \n",
    "        W = W - np.multiply(learning_rate, estimated_grad)\n",
    "        \n",
    "        new_error = residual_sum_of_squares(W,training_data_X, training_data_y)\n",
    "    \n",
    "        #Si no hubo una mejora en el error cuadrático, reducir tasa de aprendizaje (puede que se esté \"saltando\" el punto óptimo)\n",
    "        if last_error < new_error:\n",
    "            learning_rate*=0.9\n",
    "            no_improv_it = 0\n",
    "        elif last_error == new_error:\n",
    "            no_improv_it += 1\n",
    "        else:\n",
    "            no_improv_it = 0\n",
    "        \n",
    "        last_error = new_error\n",
    "        iterations += 1        \n",
    "    \n",
    "    print(\"----------------------------------------------\")\n",
    "    print(\"SDG finished, best residual_sum_of_squares %s\" % last_error)\n",
    "    print(\"W found: %s\" % W)\n",
    "    print(\" R^2 found for the model: %s\" % rcuadrado(W, training_data_X, training_data_y))\n",
    "                                                     \n",
    "def rcuadrado(W, training_data_X, training_data_y):\n",
    "    \n",
    "    residual_sum = residual_sum_of_squares(W, training_data_X, training_data_y)\n",
    "    \n",
    "    mean = np.mean(training_data_y)\n",
    "        \n",
    "    total_sum_squares = 0\n",
    "    for i in range(0, len(training_data_y)):\n",
    "        total_sum_squares +=  math.pow(training_data_y[i] - mean,2)\n",
    "    \n",
    "    print(residual_sum)\n",
    "    print(total_sum_squares)\n",
    "\n",
    "    \n",
    "    return 1 - residual_sum/total_sum_squares\n",
    "\n",
    "    \n",
    "        \n",
    "#Calcula el error de la función, se usa sólo una vez por iteración.\n",
    "def residual_sum_of_squares(W, training_data_X, training_data_y):\n",
    "    \n",
    "    sum = 0\n",
    "    for i in range(0, len(training_data_X)):\n",
    "        sum+= math.pow(training_data_y[i] - np.dot(training_data_X[i] , W.transpose())[0],2)\n",
    "    \n",
    "    return 0.5 * sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pruebas rápidas del método:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas de la matriz: 4898\n",
      "Columnas de la matriz: 12\n",
      "[[ 7.       0.27     0.36    ...  3.       0.45     8.8    ]\n",
      " [ 6.       6.3      0.3     ...  0.994    3.3      0.49   ]\n",
      " [ 9.5      6.       8.1     ... 97.       0.9951   3.26   ]\n",
      " ...\n",
      " [ 0.13     0.28     0.9     ...  0.52    11.2      6.     ]\n",
      " [ 6.7      0.48     0.49    ...  3.13     0.4     13.     ]\n",
      " [ 6.       6.7      0.48    ...  0.98926  3.13     0.4    ]]\n",
      "[6. 6. 6. ... 6. 7. 6.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data_matrix = np.loadtxt(open(\"./WineQuality/winequality-white.csv\", \"rb\"), delimiter=\";\", skiprows=1)\n",
    "print(\"Filas de la matriz: \" + str(len(data_matrix)))\n",
    "print(\"Columnas de la matriz: \" + str(len(data_matrix[0])))\n",
    "\n",
    "X = np.resize(data_matrix, (len(data_matrix), len(data_matrix[0])-1))\n",
    "y = data_matrix[:,11]\n",
    "\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "SDG finished, best residual_sum_of_squares 14751.788946528515\n",
      "W found: [[0.030339   0.0216037  0.03865635 0.01092129 0.03939777 0.03601374\n",
      "  0.02318518 0.03935858 0.03481843 0.03139065 0.0354199 ]]\n",
      "14751.788946528515\n",
      "3840.989791751859\n",
      " R^2 found for the model: -2.840621752811347\n"
     ]
    }
   ],
   "source": [
    "sgd(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pruebas con un set de datos que sé que sí sirve para regresión lineal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mglearn\n",
    "X, y = mglearn.datasets.make_wave(n_samples=5000)\n",
    "sgd(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
